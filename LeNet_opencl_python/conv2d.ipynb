{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyopencl as cl\n",
    "import pyopencl.array as cl_array\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext pyopencl.ipython_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = cl.create_some_context()\n",
    "queue = cl.CommandQueue(ctx)\n",
    "mf = cl.mem_flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1 ),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fclayer = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120,84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84,num_classes)\n",
    "        ) \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x=x.view(-1, 16*5*5)\n",
    "        x=self.fclayer(x)\n",
    "        return x\n",
    "\n",
    "model = LeNet(10)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "layer1_Conv2d = model.layer1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6, 1, 5, 5]), torch.Size([6]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_Conv2d.weight.shape,layer1_Conv2d.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(input_numpy, kernel_weight_numpy, kernel_bias_numpy, padding = 0):\n",
    "    Ci, Hi, Wi = input_numpy.shape\n",
    "    input_pad_numpy = torch.zeros(Ci, Hi+2*padding, Wi+2*padding)\n",
    "    if padding > 0:\n",
    "        input_pad_numpy[:, padding:-padding, padding:-padding] = input_numpy\n",
    "    else:\n",
    "        input_pad_numpy = input_numpy\n",
    "    Ci, Hi, Wi = input_pad_numpy.shape\n",
    "    Co, Ci, Hf, Wf = kernel_weight_numpy.shape\n",
    "    Ho, Wo = Hi - Hf + 1, Wi - Wf + 1\n",
    "    out = np.zeros((Co,Ho,Wo))\n",
    "    # conv2d weight 7 loop\n",
    "    for i in range(Ho):\n",
    "        for j in range(Wo):\n",
    "            for k in range(Co):\n",
    "                for l in range(Hf):\n",
    "                    for m in range(Wf):\n",
    "                        for n in range(Ci):\n",
    "                            out[k,i,j] += input_pad_numpy[n,i+l,j+m]*kernel_weight_numpy[k,n,l,m]\n",
    "    for i in range(Ho):\n",
    "        for j in range(Wo):\n",
    "            for k in range(Co):\n",
    "                out[k,i,j] += kernel_bias_numpy[k]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_numpy = torch.randn(1,28,28)\n",
    "weight_cpu = layer1_Conv2d.weight.detach().numpy()\n",
    "bias_cpu = layer1_Conv2d.bias.detach().numpy()\n",
    "\n",
    "padding = 2\n",
    "Ci, Hi, Wi = input_numpy.shape\n",
    "input_cpu = np.zeros((Ci, Hi+2*padding, Wi+2*padding)).astype(np.float32)\n",
    "if padding > 0:\n",
    "    input_cpu[:, padding:-padding, padding:-padding] = input_numpy\n",
    "else:\n",
    "    input_cpu = input_numpy\n",
    "    \n",
    "Ci, Hi, Wi = input_cpu.shape\n",
    "Co, Ci, Hf, Wf = weight_cpu.shape\n",
    "Ho, Wo = Hi - Hf + 1, Wi - Wf + 1\n",
    "output_cpu = np.zeros((Co,Ho,Wo)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_gpu = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = input_cpu)\n",
    "\n",
    "kernel_weight_gpu = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = weight_cpu)\n",
    "kernel_bias_gpu = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = bias_cpu)\n",
    "\n",
    "output_channel_gpu = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = np.int32(Co))\n",
    "output_height_gpu = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = np.int32(Ho))\n",
    "output_width_gpu = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = np.int32(Wo))\n",
    "input_channel_gpu = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = np.int32(Ci))\n",
    "input_height_gpu = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = np.int32(Hi))\n",
    "input_width_gpu = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = np.int32(Wi))\n",
    "feature_height_gpu = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = np.int32(Hf))\n",
    "feature_width_gpu = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf = np.int32(Wf))\n",
    "\n",
    "output_gpu = cl.Buffer(ctx, mf.WRITE_ONLY, output_cpu.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cl_kernel -o \"-cl-fast-relaxed-math\"\n",
    "\n",
    "__kernel void Conv2D(__global const float *ift, \n",
    "                     __global float *weight, __global float *bias,\n",
    "                     __global int *output_channel, __global int *output_height, __global int *output_width,\n",
    "                     __global int *input_channel, __global int *input_height, __global int *input_width,\n",
    "                     __global int *feature_height, __global int *feature_width,\n",
    "                     __global float *oft)\n",
    "{\n",
    "    int Co = *output_channel, Ho = *output_height, Wo = *output_width;\n",
    "    int Ci = *input_channel, Hi = *input_height, Wi = *input_width;\n",
    "    int Hf = *feature_height, Wf = *feature_width;\n",
    "    int posc = get_global_id(0), posh = get_global_id(1), posw = get_global_id(2);\n",
    "    int So = Wo*Ho, Sf = Wf*Hf, Si = Wi*Hi;\n",
    "    int Vf = Sf*Ci;\n",
    "    int i = posc*(So) + (posh*Wo+posw);\n",
    "    \n",
    "    oft[i] = bias[posc];\n",
    "    for(int l = 0; l < Hf; l++) {\n",
    "        for(int m = 0; m < Wf; m++) {\n",
    "            for(int n = 0; n < Ci; n++) {\n",
    "                oft[i] += ift[(n*Si)+((posh+l)*Wi)+(posw+m)]*weight[(posc*Vf)+(n*Sf)+(l*Wf)+(m)];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyopencl._cl.Event at 0x1742cb9ec48>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Conv2D(queue, input_cpu.shape, None, \n",
    "           input_gpu, \n",
    "           kernel_weight_gpu, kernel_bias_gpu,\n",
    "           output_channel_gpu, output_height_gpu, output_width_gpu,\n",
    "           input_channel_gpu, input_height_gpu, input_width_gpu,\n",
    "           feature_height_gpu, feature_width_gpu,\n",
    "           output_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyopencl._cl.NannyEvent at 0x1743c945be8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl.enqueue_copy(queue, output_cpu, output_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3761908 , -0.20601669,  0.57253695,  0.0436281 , -0.03303563,\n",
       "        0.20020744,  0.6847333 ,  1.2288802 ,  0.39604503, -0.06561807,\n",
       "       -0.7698254 ,  0.43003112,  0.43314782,  0.70534664,  0.437215  ,\n",
       "       -0.2943088 , -0.85363436, -1.5228331 , -1.1102532 , -1.29766   ,\n",
       "       -1.106854  ,  0.39128307,  1.0804993 ,  1.4540901 ,  1.6198981 ,\n",
       "        1.0882974 ,  0.58785605,  0.20178679], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_cpu[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_res = conv2d(input_numpy, weight_cpu, bias_cpu, padding = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.37619083, -0.20601666,  0.57253699,  0.04362808, -0.03303564,\n",
       "        0.2002074 ,  0.68473338,  1.22888015,  0.39604501, -0.06561805,\n",
       "       -0.76982541,  0.43003111,  0.43314783,  0.70534657,  0.43721507,\n",
       "       -0.29430877, -0.85363431, -1.522833  , -1.11025311, -1.29765989,\n",
       "       -1.10685392,  0.39128314,  1.08049928,  1.4540901 ,  1.6198983 ,\n",
       "        1.08829735,  0.58785592,  0.20178677])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_res[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
